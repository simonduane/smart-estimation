The following is intended as a reasonably complete explanation of how to do things. It's a little more abstract, and a little less focussed on the particular implementation to be found in the module and script(s) in this repo, at least compared to the [explanation](https://github.com/simonduane/smart-estimation/blob/main/docs/explanation.md) in this directory.

# Meter readings, usage and estimation

A supply produces meter readings and a history of usage. Ideally, there will be a smart meter reading every day (taken at midnight) and smart usage data for every half-hour interval, but this cannot be relied on and, if readings are missing, estimates must be made. Meter readings are obviously required for billing and, when Time of Use tariffs come in, usage data will be needed for billing too. Estimates for billing purposes **must be reliable** and it would be unacceptable for them to be wrong in an obvious way, as happens far too often for my taste with the estimates that appear as part of feedback on my energy usage.

### Some terminology, and conventions

#### Meter reading

The meter on a supply has a *register*, containing a *value* which changes with time. The value at any moment is the amount *supplied up to that moment*: the value is cumulative. A meter *reading* combines a register value with the date and time at which the register contained that value. It may be that the reading for a particular date (and perhaps also time) is needed but is not known (or was not recorded) - in that case the reading must be *estimated*.

#### Consumption (a.k.a. "usage")

An increase in the amount supplied, over a period of time, means that energy has been consumed, or used. Energy *consumption*, or energy *usage*, combines that increase in the metered quantity (i.e. the increase in the cumulative register value) with the period of time during which the energy was consumed. That period is delimited by two moments, the moments at the start and at the end of the period of time in which the energy was used. 

#### Smart meter data

Smart meters are typically set up to record usage automatically, as a set of 48 values per day where each value is the amount of energy consumed in one half-hour period. I refer to these half-hour periods as *intervals*, and will try to restrict my use of the word *period* to the case where the duration is not 30 minutes. The first of the 48 intervals starts at one midnight (UTC) and the last one ends 24 hours later, at the following midnight (UTC). The usage data for each day is retrieved by DCC more or less as soon as it is available, i.e. in the early hours of each morning. A meter reading is requested at the same time.

However, smart meter data communications are not completely reliable and this is one reason why the meter reading for a particular date and time may not be available. In the same way, the usage data for a particular date may be incomplete or even missing altogether - in that case, the usage must be *modelled*. Sometimes the modelling is well-controlled (the model is "calibrated"), other times the modelling involves significant assumptions, in particular that the pattern of usage hasn't changed since the last time the model was validated. In such a case, usage must be *projected*: a model is used, and that model may have been calibrated on previous occasions but, this time, the model is being used in a way that *does not include a current calibration*.

My aim in defining things as explicitly as this is to clarify ideas and reduce the chances of any misunderstanding. Estimation applies to meter readings, modelling applies to usage. Estimated readings are always based on modelled usage. Sometimes modelling is more reliable than other times, and this depends whether model calibration is possible. When calibration is not possible, modelling only gives a projection, or *forecast*. Generally, such a lack of calibration is a temporary state of affairs. As soon as a new actual meter reading becomes available, what were previously projections of usage coming from an uncalibrated model can be corrected by using the new reading to determine a calibration factor for the model. Replacing those uncalibrated projections of usage, with the predictions of a calibrated model of usage amounts to making a correction (retrospectively) to previous estimates.

#### A potential source of confusion: the labelling of intervals

Half-hourly usage data are associated with a large number of intervals, about 17,500 per year, all half an hour long. It would be extravagant to label every interval by two times, and so it is very common to use either the start time or the end time but not both. The source of confusion arises because data may be saved with times that mark the ends of the intervals, but read in later and used as if each time marks the start of an interval. The error is easy enough to catch when looked for but, if not checked and corrected, will persist indefinitely. That is precisely the situation Ovo finds itself in. Whether or not they are aware of the error, the error remains, still, despite being told about it months ago. I understand Ovo's data well enough that I can safely use the data and allow for this error, and the scripts in this repository do this. I can even guess how the confusion might have arisen: the moment when an interval ends is the moment that the usage *comes into existence*. Until that moment the value, how much energy would be used, was not yet definite. At the moment when the interval comes to its end, the usage so far in the interval stops changing, the value becomes fixed and, if it is known, the usage value for the interval can be recorded. I find it easy to see how, in time-stamping the new value at the moment the meter reports it, the usage becomes labelled with the time that marks the end of the interval. This thought may occur to the programmer writing code to process the usage data later, or it may not. In Ovo's case, apparently it did not because, when values are retrieved from Ovo's half-hourly usage dataset, they are labelled with both start time and end time (the extravagance is a little ironic, really), and those times are all wrong by 30 minutes.

### Model-based estimation

Estimates are best made using a model which can predict usage. The model must satisfy basic requirements, for instance that predicted usage must never exceed the supply rating (typically 25 kW for electricity, 6 m^3/hour for gas), and similarly must never be negative. But that's it, really, because the estimation process outlined here includes a *calibration* of the model[^(1)]. This calibration is what can ensure that the predictions of usage that come from the model are  consistent with the customer's actual usage. In fact, the estimates that come out can be surprisingly robust to arbitrary changes in the model.

Models may be reliable or not, but the estimates are evidenced (and reasonable, no matter the details of the model used).

[^(1)]:One way of understanding calibration is that it involves applying a special factor, a fiddle factor. This factor is the ratio of what's wanted, divided by what one already has, and "calibration" involves multiplying by this ratio (which is known as the calibration coefficient, or calibration factor). I can't think of a more general recipe for making a good correction than to multiply what you've got by what you want, and then to divide the result by what you had to start with. Think about this. Try it, and realise that it works. By all means, ask: What's the trick - it can't be that simple, surely? Well, I'm here to tell you it *is* that simple. Pearls of wisdom from a measurement standards professional of 30+ years experience, these, and all for free...

### Extrapolation, interpolation and calibration

Among all the actual meter readings for a supply, there will be an oldest reading and a newest reading, and many more readings in between. Some meter readings should be there but aren't, and these "missing" readings create gaps in the history - filling in these gaps amounts to a process of *interpolation*. Other readings may be "missing" from the period before the oldest available reading, or the latest reading may not be new enough to serve some specific purpose. Making estimates to replace missing readings such as these involves *extrapolation*. 

In the approach described here, all estimated meter readings are obtained by combining an earlier actual meter reading with the cumulative sum of usage since that earlier reading. Such an estimate is prepared for every moment that one usage interval ends and the following usage interval starts. That's an awful lot of estimates but I particularly recommend doing this, even for the times when a smart reading is already available.

The advantage in having both smart readings and model-based predicted readings available for the same time is that they can be compared with one another. The calibration process depends on such a comparison: whatever parameters the model has, the "calibration" process is effectively a way of tweaking them to improve the agreement between the model and reality. It is only the estimates that arise from making an interpolation that can benefit from such calibration. This may be obvious but, having spent over half my life working in an upmarket "calibration laboratory", perhaps it's not as obvious to others as it seems to me. Whether or not it's obvious doesn't alter the fact that it's true :-) The other estimates, the ones that involve extrapolation, are more fragile - mainly because extrapolation cannot include an automatic calibration in the way that interpolation does. Then it becomes all the more important that the model used to make the predictions that contribute to estimates, be properly validated. Calibration, where it is possible, can be regarded as model validation (if the results are carried forward in projections of future usage).

The routine production of estimates, and a systematic comparison of those estimates with reality is the best way to generate evidence of a model's validity.

## The estimation process

### Define a model

Where possible, usage values are taken from smart meter data, provided only that those data are plausible. If the usage values are implausible (I've seen a few), they should simply be ignored altogether and replaced by model-based predictions. ***A model is just a method for predicting the usage for an arbitrary 30 minute interval.*** The prediction must be ***unambiguous*** - the predicted usage might vary,it might depend on the time of day, the day of the week, or the time of year, but that's *time-dependence* which is different from *ambiguity*. In fact the model might be so trivially simple that its prediction never changes, for instance:

- "the gas usage was 0.21 kWh in every half-hour interval, on every day in Aug 2021."

But the model mustn't be stupid:

- "the gas usage between 11:30am and noon on 22 Feb 2020 was 188250.98 kWh", or

- "the gas usage on 28 Sep 2021 was -50.84kWh"

The first one, perfectly constant usage, is entirely possible, however unlikely. That's fine - it's still possible. The second one is literally impossible, and that's not fine. My gas supply is rated as U6, meaning that maximum volume of gas it can deliver in any hour is 6 m^3, which is 3 m^3 in any half-hour which, given the permitted range for the calorific value of gas supplied, turns out to be at most 32 kWh. That prediction is too large by a factor of 6000 or so. It is IMPOSSIBLE, and so to present it to me as an estimate as happened quite a few times in the two months after the meter was installed (not that the stupidly high value was aver acknowledged to be anything other than a regular usage value - it was never actually used for billing) was ***stupid***. The third one is stupid as well, because the volume of gas supplied cannot be negative, and the supplier shouldn't pretend otherwise, because that makes *them* look stupid. Even if they're coming up with a number that "balances the books" (and are mindful of possible consequences of overestimating usage) there are better ways to do this than by making the estimate unreasonably low (to the point of being negative), even for a short perioed. Suggesting that my usage is negative is unnecessary and offends me.

Those examples are real: my web browser put them on my screen at 2 pm on 18 October 2021, when I logged into my account on the My Ovo website. It was the work of a few seconds to find them - they've been there since the day those estimates were generated and they give the impression that Ovo don't understand their own data. If that's the case, I hope this repository can help them do things better.

### Estimation: the N step programme 

*How long should the list be? N=7? 10? 12?
I could make it longer if you like the style.[^(0)]*

1. Pick an actual meter reading $R_A = R(t_0)$[^(note_on_using_maths)] and, for the purpose of this explanation, it should not be the most recent one. It could even be the initial reading of zero, the value in the register when the meter was first installed. $t_0$ is just the last time that register held that value.

2. Define a model, and stick to it. See comments above for an explanation of what counts as an acceptable model.[^(2)] 

3. Use the model to calculate (i.e. predict) the usage for every half-hour interval starting with that meter reading $R(t_0)$ and going forwards[^(3)]. For convenience, in this explanation, I label each interval with the time at which it starts. So, the initial interval starts at $t_0$ and ends at $t_1$, where $t_1$ is 30 minutes later than $t_0$. Given this sequence of times, $t_i$, the model generates a corresponding sequence of values, $u_p(t_i)$. These are the model-predicted usage values, i.e. $u_p(t_i)$ is the model's prediction of the energy that was (or will be) consumed between times $t_i$ and $t_{i+1}$.[^(4)] 

4. Take the smart usage data that *are* available, and put those values in a second list. Where a smart data item is missing, be sure to put a zero in the list, in place of the missing value. Where smart usage data is available, replace the corresponding entries in the list of model predictions with zeros. The aim at this stage in the programme is to have two lists of usage values. If you put the lists alongside one another and compare corresponding entries, one or other list would be missing its value: it would be possible to merge the two lists into one list, because precisely one of the lists has a value to use - the value in the other list is missing, and has been replaced by zero. Zero is a potentially confusing value to insert, but its convenience is that the merging of lists becomes a matter of just adding them up, list position by list position. The possible confusion arises because - what if a smart usage data item exists and its value is *actually* zero? All I'd say is, take care. Actual usage can really be zero but model-predicted usage cannot, because at some point that model predicted value might appear in the denominator of a calibration factor, and dividing by zero is a no-no. Having both lists actually resolves a potential ambiguity, because the only way the list of model predictions can have a zero is if the prediction for that time has been replaced by zero, which means there must be genuine smart data - even if that smart value does turn out to be zero :-)[^(5)]

5. Make a third list by merging these two, $u(t_i) = u_s(t_i) + u_p(t_i)$, taking either the smart value or the model-predicted value (in practice, just add them up, term by term). Yes, if they're both zero that's fine - the model value has been replaced by zero and the smart value really is zero (and is the one that counts).

6. Calculate a fourth list, in which the initial entry is $R(t_0)$ and the later entries satisfy $R(t_{i+1}) = R(t_i) + u(t_i)$. This is the list of projected readings. Each projected reading is the initial actual reading plus the cumulative sum of smart usages plus the cumulative sum of modelled usages.

7. Eventually, because $R_A$ was not the most recent actual reading, the intervals will reach the next actual reading, $R_B=R(t_n)$. Note the sleight of hand here: I'm assuming that the actual readings $R_A$ and $R_B$ were taken at times $t$ and $t'$ that are both among the moments that separate adjacent usage intervals. I will stick with that approximation, even though I know it's not always true (a check can be designed, I made the check, and found that it wasn't true.). If you think that's a problem, come back to me when you have a method that doesn't make this approximation and doesn't doesn't also produce stupid estimates. I've no doubt that such a method exists, but my preference is to focus on the important problems (and implement their easy solution) first.

8. At this stage, we have a pair of actual readings, $R_A$ and $R_B$ and a whole series of what I'm referring to as projected readings, $R(t_i)$, of which the last one, $R(t_n)$ should be the same as the actual reading taken at that time, $R_B$. From this information, we calculate a calibration factor for the model. When we include this calibration factor in the calculation already described, it turns out that $R(t_i) = R_B$, because that's what the calibration factor is required to achieve. We do this by isolating the contribution that the model predictions make to this estimate, and scale that contribution by whatever factor is needed to make the estimate come out right. It's a trivial calculation, to be quite honest. That factor is the calibration factor for the model and, for every interval in the period under consideration, we say that the best model-based ***estimate*** is given by the same process as before (steps 5-6), but in which the model-predicted contributions have all been multiplied by the same ***calibration factor***.

9. That's an awful lot of estimated readings. One for every half-hour in that period. If the period straddles more than one day, then the intermediate moment(s) of midnight are times when we were lacking a smart meter reading. We are still lacking an actual smart reading, but you have just read an explanation of how to make the best estimate you can.

10. The final step in this programme is to apply the programme, to translate the programme into a program.

[^(0)]:But it'll never come out in **Paperback**. Please forgive the cultural reference - I'm a child of the Sixties.

[^(note_on_using_maths)]:Unfortunately, GitHub doesn't display the maths formulae I have used here (it's like a simplified version of TeX or LaTeX). Other markdown editors do and so I have left the formulae as they are (they're not totally illegible as plain text) in case the reader sees this file in some other markdown renderer.

[^(2)]:It turns out that there is a technical restriction, that if a usage could in principle be non-zero, then the model prediction had better not be zero. This is because the calibration process in step 8 below involves dividing by a sum of model-predicted usage values. If the sum is only over ones that are definitely zero, this doesn't make sense. Such a model can't be fixed using a multiplicative calibration factor. So, make sure that the problem doesn't arise by only using models that predict usage that is strictly positive. These are the ones that can always be calibrated in this way.

[^(3)]:Starting from a later reading, it is possible to go backwards instead. A similar logic applies, but it's potentially confusing to have both directions in your mind at the same time, especially at first, when it's all new anyway. Instead, get your head around the forwards case, completely, before asking yourself - how would this work going in the other direction...

[^(4)]:The question, is it better to label an interval by its start time or by its end time, has no right answer, and there is room for confusion. In this note, usages are labelled with the interval start time. The key thing is consistency, and that is facilitated by clarity. Otherwise one can end up in the position where usage is labelled as having such and such a start time when, as a matter of fact, I know that that time is at the end of the usage interval. In using data that comes from such a source it takes an extra special effort to be sure that mistakes are not being made...

[^(5)]:There is a huge difference between a reading being missing, and a reading happening to have the value zero. That sounds obvious, put like that, but you might be surprised how often the distinction is not maintained. Hildebrand seem particularly clueless in this regard: anything more recent than the latest smart data is left blank, but if subsequent smart data is retrieved creating a gap (i.e. missing data) the Bright app just fills that gap with zeros. In the terms of this explanation, it's as if the model they are using predicts zero. Perhaps a better way of putting it is that they just don't have a model for estimating usage (or, if they have one, they don't make it public). Incidentally, I realise that the list of model-predicted values can be used to stand in for a logical flag whose meaning is "smart data missing here", by evaluating the boolean expression "model prediction is strictly greater than zero" as true/false.

## Postscript

There is room for playing here. The only constraint on the model is that it doesn't produce stupid predictions, ones which are beyond the physical limits of what's possible. What the model will never do is correctly predict the random variations in usage that happen from half-hour to half-hour and from one day to another. So, in validating a model against actual usage, don't be fooled into taking the patterns in actual usage too seriously. Some of them are just statistical noise. Taking those small random variations seriously enough to build them into the model is another kind of stupidity, and Ovo's current estimation process displays that stupidity too.

What's more fun is to design models that actually do a reasonable job of predicting usage. It turns out that (provided one is careful not to take those small random variations seriously) this process of calibration will generate a good model automatically. Tune in next time to find out how...
