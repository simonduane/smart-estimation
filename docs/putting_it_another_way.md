# Meter readings, usage and estimates

A supply produces meter readings and a history of usage. Ideally, there will be a smart meter reading every day (at midnight) and smart usage data for every half-hour interval, but this cannot be taken for granted and, if data are missing, estimates must be made. Meter readings are obviously required for billing and, when Time of Use tariffs come in, usage data will be needed for billing too. Estimates that are used in billing must be reliable and it will be unacceptable for them to be wrong in an obvious way as, at the moment, seems to be happening regularly with the estimates that appear as part of feedback to users on their usage.

## Model-based estimation

Estimates are best made using a model which can predict usage. The model must satisfy basic requirements, for instance that predicted usage must never exceed the supply rating (typically 25 kW for electricity, 6 m^3/hour for gas), and similarly must never be negative. But that's it, really, because the estimation process outlined here includes a *calibration* of the model[^(a)]. This calibration is what *can* make sure that model, i.e. all its predictions, are consistent with the customer's actual usage. In fact, the estimates can be surprisingly robust.
[^(a)]:One way of understanding calibration is that it involves using a special factor, a fiddle factor. This  factor is the ratio of what's wanted, divided by what one already has, and "calibration" involves multiplication by the calibration factor. I can't think of a more general recipe for making a correction than to multiply what you've got by what you want, and then to divide the result by what you had to start with. First you think about it, then you try it and realise it works, then you wonder: what's the trick - it can't be that simple, surely? Well, I'm here to tell you it *is* that simple. Pearls of wisdom from a professional of 30+ years experience, these, and all for free...

## Extrapolation, interpolation and calibration

Among all the actual meter readings for a supply, there will be an oldest reading and a newest reading, and many more readings in between. Some meter readings should be there but aren't, and these "missing" readings create gaps in the history - filling in these gaps amounts to a process of *interpolation*. Other readings may be "missing" from the time before the oldest available reading, or the newest reading may not be new enough to serve some purpose or other. Making estimates to replace missing readings such as these involves *extrapolation*. 

In the approach described here, all estimated meter readings are obtained by combining an earlier actual meter reading and with the cumulative sum of usage since that earlier reading. Such an estimate is prepared for every moment that one usage interval ends and the following usage interval starts. That's an awful lot of estimates and I particularly recommend doing this, even for times when a smart reading is already available.

The advantage in having both smart readings and model-based predicted readings available for the same time is that they can be compared with one another. The calibration process depends on such a comparison: whatever parameters the model has, "calibration" effectively is a way of tweaked them to improve the agreement between the model and reality. It is only estimates that amount to interpolation that can benefit from such calibration. This may be obvious but, having spent over half my life working in an upmarket "calibration laboratory", perhaps it's not as obvious as I like to think. Whether or not it's obvious doesn't alter the fact that it's true :-) . The other estimates, the ones that involve extrapolation, are more fragile - mainly because extrapolation cannot include an automatic calibration the way that interpolation does. In that case it becomes all the more important that the model, used to make the predictions that contribute to estimates, be properly validated. Calibration, where it is possible, is part of validation.

The routine production of estimates, and a systematic comparison of those estimates with reality is the best way to generate evidence of a model's validity.

## The estimation process

### Define a model

Where possible, usage values are taken from smart meter data, provided only that those data are plausible. If the usage values are implausible (I've seen a few), they should simply be ignored altogether and replaced by model-based predictions. ***A model is just a method for predicting the usage for an arbitrary 30 minute interval.*** The prediction must be ***unambiguous*** - the predicted usage might vary,it might depend on the time of day, the day of the week, or the time of year, but that's *time-dependence* which is different from *ambiguity*. In fact the model might be so trivially simple that its prediction never changes, for instance

- "the gas usage was 0.21 kWh in every half-hour interval, on every day in Aug 2021"

but the model mustn't be stupid 

- "the gas usage between 11:30am and noon on 22 Feb 2020 was 188250.98 kWh", or
- "the gas usage on 28 Sep 2021 was -50.84kWh"

The first one, perfectly constant usage, is entirely possible, however unlikely. That's fine, it's still possible. The second one is literally impossible, and that's not fine. My gas supply is rated as U6, meaning that maximum volume of gas it can deliver in any hour is 6 m^3, which is 3 m^3 in any half-hour which, given the permitted range for the calorific value of gas supplied, turns out to be at most 32 kWh. That prediction is too large by a factor of 6000 or so. It is IMPOSSIBLE, and so to present it to me as an estimate as happened quite a few times early on (not that it was even admitted to be anything other than a regular usage value) was ***stupid***. The third one is stupid as well, because the volume of gas supplied cannot be negative, and the supplier shouldn't pretend otherwise, because that makes them look stupid. Even if they're coming up with a number that "balances the books" there are other ways to do that and, as a way of balancing the books, making usage negative offends me.

Those examples are real: my web browser put them on my screen at 2 pm on 18 October 2021, when I logged into my account on the My Ovo website. It was the work of a few seconds to find them - they've been there since the day those estimates were generated and they are among the reasons I'm currently disinclined to renew my account with Ovo. Once Ovo's system software has shown itself to be untrustworthy, why ever would I want renew my Ovo account?

### Estimation: the N step program 

*What should N be? 10? 12? I could make it whatever you want*

1. Pick an actual meter reading $R_A=R(t_0)$ and, for the purpose of this explanation, it should not be the most recent one. It could even be that initial reading of zero that was on the meter when it was first installed. $t_0$ is just the last time that value was in the meter's register.
2. Define a model, and stick to it. See the previous comments for an explanation of what counts as a model.[^(0)] 
3. Use the model to calculate (i.e. predict) the usage for every half-hour interval starting with that meter reading and going forwards. (You can go backwards instead. A similar logic would apply, but it's potentially confusing to have both directions in your mind at the same time, especially at first, when it's all new anyway. Instead, get your head around the forwards case, completely, before asking yourself - how would this work going in the other direction...) For convenience, label each interval with the time at which it ends. So, the first interval starts at $t_0$ and ends at $t_1$, and $t_1$ is 30 minutes later than $t_0$. The model generates a list, or sequence, of values $u_p(t_i)$, i.e. the predicted usage, the model's prediction of the energy that was (or will be) consumed between times $t_i$ and $t_{i+1}$.[^(1)] 
4. In those intervals where a smart usage data item is available, $u_s(t_i)$, replace the model prediction with a zero, and create a second list. The second list has all the smart usage data, and it has zeros wherever the smart meter usage value is missing. At this stage in the program, we have two lists of usage values. If you put the lists alongside one another, at any position in the list (i.e. for any interval) exactly one of the two lists has a zero.
5. Make a third list, which has the non-zero list entry, smart or model-predicted, in every slot.
6. Calculate a fourth list, in which the initial entry is $R_0$ and the later entries satisfy $R_{i+1}=R_i+u_i$. In this expression, you just have to take $u_i$ from whichever list has the value that hasn't been replaced with zero.[^(2)] 
7. Eventually, because $R_A$ was not the most recent actual reading, the intervals will reach the next actual reading, $R_B=R(t_n)$. Note the sleight of hand here: I'm assuming that the actual readings $R_A$ and $R_B$ were taken at times $t$ and $t'$ that are both among the moments that separate adjacent usage intervals. I will stick with that approximation. Come back to me when you have a method that doesn't make this approximation and but doesn't also doesn't produce stupid estimates. I've no doubt that such a method exists, but fix the important and easy problems first, eh?
8. At that point, we have a pair of actual readings, $R_A$ and $R_B$ and a whole series of estimated readings, $R(t_i)$, of which the last one, $R(t_n)$ should be the same as the actual reading taken at that time, $R_B$. From this information, we calculate a calibration factor for the model, one which ensures that when we use it to make the best estimate we can, the estimated meter reading agrees precisely with the actual meter reading. We do this by isolating the contribution that the model made to this estimate, and scaling that contribution by whatever factor is needed to make the estimate correct. It's a trivial calculation, honestly. That factor is the calibration factor for the model and, for every interval in the period under consideration, we say that the best model-based ***estimate*** is the ***prediction*** we had before, but multiplied by this ***calibration factor***.
9. That's an awful lot of estimated readings. One for every half-hour in that period. If the period straddles more than one day, then the intermediate moment(s) of midnight are times when we were lacking a smart meter reading. We are still lacking a smart reading, but you have just read an explanation of how to make the best estimate you can.
10. The final step in this programme is to apply the programme, to translate the programme into a program.

## Postscript

There is room for playing here. The only constraint on the model is that it doesn't produce stupid predictions, ones which are beyond the physical limits of what's possible. What the model will never do is correctly predict the random variations in usage that happen from half-hour to half-hour and from one day to another. So, in validating a model against actual usage, don't be fooled into taking the patterns in actual usage too seriously. Some of them are just statistical noise. Taking those small random variations seriously enough to build them into the model is another kind of stupidity, and Ovo's current estimation process displays that stupidity too.
[^(0)]:It turns out that there is a technical restriction, that if a usage could in principle be non-zero, then the model prediction had better not be zero. This is because the calibration process in step 8 below involves dividing by a sum of model-predicted usage values. If the sum is only over ones that are definitely zero, this doesn't make sense. So define the problem away by making sure that no predicted usage is precisely zero, even if that happens quite often. The division is by the prediction, not by what actually happens.
[^(1)]:This question, whether an interval is best labelled by its start or its end, is one that can cause no end of confusion. In this note, the usages are labelled the interval start time. Just don't use it as if it's the start time when, it truth, the time is actually at the end of the interval. You can be better than Ovo, really you can... 
[^(2)]:There is a huge difference between a reading being missing, and a reading happening to have the value zero. That sounds obvious, put like that, but you might be surprised how often the distinction is not maintained. I find Hildebrand are hopeless in this regard: anything more recent than the latest smart data is left blank, but if subsequent smart data is retrieved creating a gap (i.e. missing data) the Bright app just shows it as zero. In the terms of this explanation, it's as if the model they are using predicts zero. 
